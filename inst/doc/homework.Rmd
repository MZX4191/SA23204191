---
title: "homework"
author: "By SA23204191 马子骁"
date: "2023-12-02"
output: html_document
vignette: >
  %\VignetteIndexEntry{Introduction to R-package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# homework 1
## Question 
Use knitr to produce at least 3 examples. For each example,texts should mix with figures and/or tables. Better to have mathematical formulas.

## Answer

## Example 1
```{r}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef

```
The $R^2$ is `r summary(lm.D9)$r.squared`

Execute but not show R code
```{r,echo=FALSE}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef
```
The $R^2$ is `r summary(lm.D9)$r.squared`

Show but not execute R code
```{r,eval=FALSE}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef
```
The $R^2$ is `r summary(lm.D9)$r.squared`

Neither show nor execute R code
```{r,echo=FALSE,eval=FALSE}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
summary(lm.D9)$coef
```
## Example 2
```{r}
xtable::xtable(head(iris))
a<-xtable::xtable(head(iris))
xtable::print.xtable(a,type="latex",file="D:/Datafiles/1.tex" ) #将生成的Latex代码保存
knitr::kable(head(iris)) #也可以直接用kable()生成表格
```
## Example 3
```{r}
par(mfrow=c(2,2))
plot(lm.D9)
```

# homework 2

## Question 1
利用逆变换法复现函数sample的部分功能(replace=TRUE)

## Answer
```{r }
my.sample<-function(x,size,prob=1/length(x))
{u<-runif(size)
cp <- cumsum(prob)
a<-x[findInterval(u,cp)+1]
a
}
```
一些例子
```{r }
my.sample(0:1,10)
my.sample(1:4,50,c(0.1,0.2,0.3,0.4))
```



## Question 2
The standard Laplace distribution has density $f(x)=\frac{1}{2}e^{-|x|},x\in\mathbb{R}$.Use the inverse transform method to generate a random sample of size 1000 from this distribution. Use one of the methods shown in this chapter to compare the generated sample to the target distribution.

## Answer
```{r}
n <- 1000
u <- runif(n)
x<-numeric((1000))
x[u>=1/2]<--log(2*(1-u)) # F(x)=(e^x)/2,x<0
x[u<1/2]<-log(2*u) # F(x)=1-(e^-x)/2,x>=0      
x
hist(x, prob = TRUE,ylim =c(0, 0.5))
y <- seq(-5, 5, .01)
lines(y,exp(-abs(y))/2 )

```



## Question 3
Write a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.

## Answer
```{r}
betasample<-function(n,a,b) #这里a,b的值应该大于等于1
{j<-k<-0;y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1) 
if (x^(a-1) * (1-x)^(b-1) > u) {
k <- k + 1
y[k] <- x
}
}
y
} 
z<-betasample(1000,3,2)
z
hist(z, prob = TRUE,ylim =c(0, 2))
x <- seq(0, 1, .01)
lines(x,dbeta(x,3,2))
```



## Question 4
The rescaled Epanechnikov kernel is a symmetric density function
$$
f_e(x)=\frac{3}{4}(1-x^2),|x|\leq1
$$
Devroye and Gy¨orfi give the following algorithm for simulation from this distribution. Generate iid $U_1, U_2, U_3$ ∼ Uniform(−1, 1). If $|U_3| \geq|U_2|$and $|U_3| \geq|U_1|$, deliver $U_2$;otherwise deliver $U_3$. Write a function to generate random variates from $f_e$, and construct the histogram density estimate of a large simulated random sample.

## Answer
```{r}
fesample<-function(n){
x<-numeric(n)
i<-1
for(i in 1:n){
u<-runif(3,-1,1)
if(abs(u[3])>=abs(u[2])&abs(u[3])>=abs(u[1]))
{x[i]<-u[2]}
else{x[i]<-u[3]}
i<-i+1
}
x
}
y<-fesample(1000)
y
hist(y, prob = TRUE,ylim =c(0, 1))
x <- seq(-1, 1, .01)
lines(x,3*(1-x^2)/4 )
```



## Question 5
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e$ .

## Answer
$P(X\leq x)=P(U_2\leq x,|U_3|\geq|U_2|,|U_3|\geq|U_1|)+P(U_3\leq x,|U_3|<|U_2|or|U_3|<|U_1|)$
$=P(U_2\leq x,|U_3|\geq|U_2|)P(|U_3|\geq|U_1|)+P(U_3\leq x,|U_3|<|U_2|or|U_3|<|U_1|)$
$=\frac{3}{4}x-\frac{1}{4}x^3$

# homework 3

## Question 1

Proof that what value $\rho=\frac{l}{d}$ should take to minimize the asymptotic variance of $\widehat{\pi}$? ($m\thicksim B(n,p)$,using $\delta$ method)
Take three different values of $\rho (0 ≤ \rho ≤ 1, including\ \rho_{min})$ and use Monte Carlo simulation to verify your answer. ($n = 106$, Number of repeated simulations $K = 100$)

## Answer

由于$\widehat\pi$的方差较难计算，我们可以转而计算$\frac{1}{\widehat\pi}$的方差
$$
\begin{aligned}
Var(\frac{1}{\widehat\pi})&=Var(\frac{1}{2\rho}\frac{m}{n})\\
&=\frac{1}{2n\pi\rho}-\frac{1}{n\pi^2}
\end{aligned}
$$
 $\rho$应尽可能大，故$\rho=1$
```{r}
rho1<-1
rho2<-1/2
rho3<-1/3
m <- 1e6
pihat1<-numeric(100) #rho=1时的模拟结果
pihat2<-numeric(100) #rho=1/2时的模拟结果
pihat3<-numeric(100) #rho=1/3时的模拟结果
for(i in 1:100)
{
X <- runif(m,0,1/2)
Y <- runif(m,0,pi/2)
pihat1[i]<- 2*rho1/mean(rho1/2*sin(Y)>X)
pihat2[i] <- 2*rho2/mean(rho2/2*sin(Y)>X)
pihat3[i] <- 2*rho3/mean(rho3/2*sin(Y)>X)
}
var(pihat1)
var(pihat2)
var(pihat3)
```

## Question 2

 In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of
$$\theta=\int_0^1 e^x {\rm d}x$$
Now consider the antithetic variate approach. Compute $Cov(e^U , e^{1-U} )$ and 
$Var(e^U + e^{1−U} )$, where $U\thicksim Uniform(0,1)$. What is the percent reduction in variance of $\widehat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

## Answer

$$
\begin{aligned}
Cov(e^U , e^{1-U} )&=E[(e^U-E[e^U])(e^{1-U}-E[e^{1-U}])]\\
&=-e^2+3e-1
\end{aligned}
$$
$$
\begin{aligned}
Var(e^U + e^{1−U} )&=Var(e^U)+Var(e^{1-U})+2Cov(e^U , e^{1−U} )\\
&=-3e^2+10e-5
\end{aligned}
$$
```{r}
n<-1e4
var1<-1/n*(2*exp(1)-1/2*exp(2)-3/2)
var2<-1/2/n*(10*exp(1)-3*exp(2)-5)
a<-(var1-var2)/var1
```
$\widehat{\theta}$的方差为$\frac{1}{n}Var(e^U)$,
使用对偶变量后$\widehat{\theta}$的方差为$\frac{1}{2n}Var(e^U+e^{1-U})$,
$n=10000$时,减少了`r a`

## Question 3

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

## Answer

```{r}
n<-1e4
x<-runif(n)
theta.hat<-mean(exp(x)) #simple Monte Carlo method
theta.hat1<-mean(exp(x[1:n/2])+exp(1-x[1:n/2]))/2 #antithetic variate approach
var1<-1/n*var(exp(x))
var2<-var(exp(x[1:n/2])+exp(1-x[1:n/2]))/2/n
a<-(var1-var2)/var1
```
用普通MC方法得出的$\widehat{\theta}$是`r theta.hat`,用对偶变量方法得出的$\widehat{\theta}_1$是`r theta.hat1`,方差减小了`r a`,与Question2得出的理论值差不多

# homework 4

## Question 1
$Var(\widehat{\theta}^M)=\frac{1}{Mk}\sum_{i=1}^{k}\sigma_i^2+\frac{1}{M}Var(\theta_I)=Var(\widehat{\theta}^S)+\frac{1}{M}Var(\theta_I)$,where $\theta_i=E[g(U)|I=i],\sigma_i^2=Var[g(U)|I=i]$and $I$ takes uniform distribution over$\left\{1,...,k\right\}$.  
Proof that if g is a continuous function over$(a,b)$,then $Var(\widehat{\theta}^S)/Var(\widehat{\theta}^M)\rightarrow 0$ as $b_i-a_i \rightarrow 0$ for all $i=1,...,k$ .

## Answer
$$\begin{align} Var(\theta_I)&=Var(E[g(U)|I])\\  
&=E[E[g(U)|I]^2]-E[E[g(U)|I]]^2\\
&=\frac{1}{k}\sum_{i=1}^{k}(\int_{a_i}^{b_i}g(u)\frac{1}{b_i-a_i}\mathrm{d}u)^2-E[g(U)]^2\\
由g的连续性，&存在u_i\in(a_i,b_i),  使得\int_{a_i}^{b_i}g(u)\frac{1}{b_i-a_i}\mathrm{d}u=g(u_i),\\
Var(\theta_I)&=\frac{1}{k}\sum_{i=1}^{k}g(u_i)^2-E[g(U)]^2\\
&\rightarrow Var(g(U))\quad as\quad b_i-a_i\rightarrow 0\\
注意到Var(\widehat{\theta}^M)=&Var(\widehat{\theta}^S)+\frac{1}{M}Var(\theta_I)=\frac{1}{M}Var(g(U)),故Var(\widehat{\theta}^S)\rightarrow 0\quad as\quad b_i-a_i\rightarrow 0.\\
所以Var(\widehat{\theta}^S)/Var&(\widehat{\theta}^M)\rightarrow 0\quad as\quad b_i-a_i \rightarrow 0
\end{align}$$



## Question2 (5.13 5.14)
### 5.13
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and
are ‘close’ to$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}, \qquad x>1$$Which of your two importance functions should produce the smaller variance in estimating$$\int_{1}^{\infty} \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2} \mathrm d x$$by importance sampling? Explain.

### 5.14
Obtain a Monte Carlo estimate of$$\int_{1}^{\infty} \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2} \mathrm d x$$by importance sampling.

## Answer
$$\begin{align}&f_1(x)=\sqrt{e}xe^{-x^2/2},\qquad x \in [1,\infty)\\
&f_2(x)=\frac{1}{0.1589}\frac{1}{\sqrt{2\pi}}e^{-x^2/2},\qquad x \in [1,\infty)\end{align}$$
```{r}
f<-function(x){x^2*exp(-(x^2)/2)/sqrt(2*pi)}
f1<-function(x){sqrt(exp(1))*x*exp(-(x^2)/2)}
n<-1e5
u<-runif(n)
x<-sqrt(1-2*log(1-u))#用 Inverse transform method生成密度f1的随机数
est1<-mean(x/sqrt(2*pi*exp(1)))
var1<-1/n*var(x/sqrt(2*pi*exp(1)))
y<-rnorm(ceiling(n/0.1589*10))
y <- y[y > 1]
y <- y[1:n]#生成标准正态随机数并保留大于1的
est2<-mean(0.1589*y^2)
var2<-1/n*var(0.1589*y^2)
```
当权重函数为$f_1$时，估计为`r est1`,方差近似为`r var1`  
当权重函数为$f_2$时，估计为`r est2`,方差近似为`r var2`  
$f_1$的方差更小，因为$f_1$与原积分函数比值为$c_1x$,$f_2$与原积分函数比值为$c_2x^2$,$(c_1,c_2为常数)$

## Question 3
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

## Answer
```{r}
M <- 10000; k <- 5
T<-numeric(k)
a<-numeric(k)
var<-numeric(k)
g<-function(x){exp(-x)/(1+x^2)*(x>0)*(x<1)}
for(j in 1:k){
  u<-runif(M/k,(j-1)/k,j/k)
  a[j]<-(exp((1-j)/k)-exp(-j/k))/(1-exp(-1))
  x<--log(exp((1-j)/k)-(1-exp(-1))*u*a[j])
  fg<-g(x)/(exp(-x)/(1-exp(-1)))
  T[j]<-mean(fg)
  var[j]<-var(fg)
}
est1<-a%*%T
sd1<-sqrt(a^2%*%var*k/M)#stratified importance sampling estimate and its sd

u <- runif(M)
x <- - log(1 - u * (1 - exp(-1)))
fg <- g(x) / (exp(-x) / (1 - exp(-1)))
est2<- mean(fg)
sd2<- sd(fg)/sqrt(M)#importance sampling estimate and its sd
```
用分层权重抽样的估计值是`r est1`,标准差是`r sd1`  
仅用权重抽样的估计值是`r est2`,标准差是`r sd2`

## Question 4
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n=20$. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

## Answer
```{r}
m<-1000
n<-20
L<-numeric(m)
U<-numeric(m)
UCL<-numeric(m)
for(i in 1:m){
x<-rchisq(n,2)
L[i]<-mean(x)-sd(x)/sqrt(n)*qt(0.975,n-1)
U[i]<-mean(x)+sd(x)/sqrt(n)*qt(0.975,n-1)
}
mean(L<=2 & 2<=U)
for(i in 1:m){
x<-rchisq(n,2)
UCL[i] <- (n-1) * var(x) / qchisq(0.05, df=n-1)
}
mean(4<=UCL)
```
在模型错误的情况下,期望的置信区间覆盖率为`r mean(L<=2 & 2<=U)`,方差的置信上限覆盖率为`r mean(4<=UCL)`,t-置信区间更具稳健性

## Question 5
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform(0,2), and (iii)Exponential(rate=1). In each case, test $H_0 : \mu = \mu_0\quad vs\quad H_0 : \mu 
\neq \mu_0$, where $\mu_0$ is the mean of $\chi^2(1)$, Uniform(0,2), and Exponential(1), respectively.

## Answer
```{r}
mu0<-1
alpha<-0.05
n<-20
m<-1e4
p<-numeric(m)
for(i in 1:m){
x<-rchisq(n,1)
p[i]<-t.test(x,mu=mu0)$p.value
}
mean(p<=alpha)

for(i in 1:m){
x<-runif(n,0,2)
p[i]<-t.test(x,mu=mu0)$p.value
}
mean(p<=alpha)

for(i in 1:m){
x<-rexp(n,1)
p[i]<-t.test(x,mu=mu0)$p.value
}
mean(p<=alpha)
```
从结果看只有$[0,2]$上的均匀分布的第一类错误概率接近预先给定的0.05,因为其密度关于期望$\mu_0$对称

# homework 5

## Question 1
考虑$m=1000$个假设，其中前$95\%$个原假设成立，后$5\%$个对立假设成立。在原假设之下，p值服从$U(0,1)$ 分布，在对立假设之下，p值服从$Beta(0.1,1)$分布(可用rbeta生成)。应用Bonferroni校正与B-H校正作用于生成的m个p值(独立)(应用p.adjust)，得到校正后的p值，与$\alpha=0.1$比较确定是否拒绝原假设。基于$M=1000$次模拟，可估计FWER,FDR,TPR输出到表格中。

## Answer
```{r}
alpha<-0.1
m<-1000
M<-1000
p<-numeric(m)
p.adj1<-matrix(0,m,M)
p.adj2<-matrix(0,m,M)
FWER<-matrix(0,M,2)
FDR<-matrix(0,M,2)
TPR<-matrix(0,M,2)

for(i in 1:M){
 p1<-runif(.95*m) #negative
 p2<-rbeta(.05*m,0.1,1) #positive
 p<-sort(c(p1,p2))
 p.adj1[i,] <- p.adjust(p,method='fdr')
 p.adj2[i,] <- p.adjust(p,method='bonferroni')
 FWER[i,1]<-sum(p.adj1[i,]<=alpha&p%in%p1)!=0
 FWER[i,2]<-sum(p.adj2[i,]<=alpha&p%in%p1)!=0
 FDR[i,1]<-sum(p.adj1[i,]<=alpha&p%in%p1)/sum(p.adj1[i,]<=alpha)
 FDR[i,2]<-sum(p.adj2[i,]<=alpha&p%in%p1)/sum(p.adj2[i,]<=alpha)
 TPR[i,1]<-(sum(p.adj1[i,]<=alpha)-sum(p.adj1[i,]<=alpha&p%in%p1))/.05/m
 TPR[i,2]<-(sum(p.adj2[i,]<=alpha)-sum(p.adj2[i,]<=alpha&p%in%p1))/.05/m
}
a<-data.frame(FWER=round(apply(FWER,2,mean),3),FDR=round(apply(FDR,2,mean),3),TPR=round(apply(TPR,2,mean),3),row.names = c("B-H","Bonferroni"))

knitr::kable (a)


```

## Question 2
Suppose the population has the exponential distribution with rate$\lambda$ , then the MLE of $\lambda$ is $\widehat{\lambda}=\frac{1}{\overline{X}}$ , where $\overline{X}$ is the sample mean. It can be derived that the expectation of$\widehat{\lambda}$ is $\frac{n}{n-1}\lambda$,
so that the estimation bias is $\frac{λ}{n − 1}$. The standard error$\widehat{\lambda}$  is
$\frac{nλ}{(n-1)\sqrt{n-2}}$. Conduct a simulation study to verify the performance of the bootstrap method.  
The true value of λ = 2.  
The sample size n = 5, 10, 20.  
The number of bootstrap replicates B = 1000.  
The simulations are repeated for m = 1000 times.  
Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.

## Answer
```{r}
lambda<-2
n<-c(5,10,20) 
B<-1000 
m<- 1000  
lambda.hat<-matrix(0,m,length(n))
bias<-matrix(0,m,length(n))
se<-matrix(0,m,length(n))
MLE<-function(x,i){
 return(1/mean(x[i]))
}
for(j in 1:length(n)){ 
 for(i in 1:m){
  x<-rexp(n[j],lambda)
  obj<-boot::boot(data=x,statistic=MLE,R=B)
  lambda.hat[i,j]<-obj$t0
  bias[i,j]<-mean(obj$t)-obj$t0
  se[i,j]<-sd(obj$t)
 }
}
a<-data.frame(bias_theoretical=round(lambda/(n-1),3),bias_bootstrapmean=round(apply(bias,2,mean),3),se_theoretical=round(n*lambda/(n-1)/sqrt(n-2),3),se_bootstrap=round(apply(se,2,mean),3),row.names = c("n=5","n=10","n=20"))
knitr::kable (a)
```

## Question 3
Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap).

## Answer
```{r}
library(bootstrap)
B <- 500
n <- nrow(law)
R <- 100
alpha<-.05
cor<-numeric(B)
cor2<-numeric(R)
t<-numeric(B)
corhat<-cor(law$LSAT, law$GPA)
for (b in 1:B) {
 x <- sample(1:n, size = n, replace = TRUE)
 LSAT <- law$LSAT[x] 
 GPA <- law$GPA[x]
 cor[b] <- cor(LSAT, GPA)
 for(i in 1:R){
   y<- sample(x, size = n, replace = TRUE)
   LSAT <- law$LSAT[y] 
   GPA <- law$GPA[y]
   cor2[i] <- cor(LSAT, GPA)
 }
 t[b]<-(cor[b]-corhat)/sd(cor2) 
}
t1<-corhat-sort(t)[ceiling(B*(1-alpha/2))]*sd(cor)
t2<-corhat-sort(t)[floor(B*(alpha/2))]*sd(cor)
```
一个`r 1-alpha`的t置信区间为[`r t1`,`r t2`]

# homework 6

## Question 1
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures 1/λ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.

## Answer
```{r }
library(boot)
set.seed(123)
R<-1000
b.mean<-function(x,i){
  mean(x[i])
}
obj<-boot(aircondit$hours,b.mean,R)
ci<-boot.ci(obj,type=c("norm","basic","perc","bca"))
ci
```
Normal与basic有差别是由于$\widehat{\frac{1}{\lambda}}$不服从正态分布  
Percentile与basic有差别是由于$\frac{\widehat{\frac{1}{\lambda}_{\alpha/2}^*}+\widehat{\frac{1}{\lambda}_{1-\alpha/2}^*}}{2}\neq\widehat{\frac{1}{\lambda}}$  
BCa与Percentile有差别是由于不存在单调变换$F$使得$F(\widehat{\frac{1}{\lambda}})$服从正态分布

## Question 2
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\widehat{\theta}$.

## Answer
```{r}
library(bootstrap)
x<-as.matrix(scor)
n<-nrow(scor)
m<-ncol(scor)
b.cov<-function(x,i){
  cov(x[i,])
}
COV<-matrix(0,m,m)
theta.hat <-eigen(cov(x))$values[1]/sum(eigen(cov(x))$values)
theta.jack <- numeric(n)
for(i in 1:n){
COV <- b.cov(x,(1:n)[-i])
theta.jack[i] <- eigen(COV)$values[1]/sum(eigen(COV)$values)
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)

```

## Question 3
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

## Answer
```{r}
library(DAAG); attach(ironslag)
n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- matrix(0,n,n)
for (i in 2:n) {
  for(j in 1:(i-1)){
  
  y <- magnetic[-c(i,j)]
  x <- chemical[-c(i,j)]
  
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[i]
  yhat2 <- J1$coef[1] + J1$coef[2] * chemical[j]
  e1[i,j] <- magnetic[i] - yhat1
  e1[j,i] <- magnetic[j] - yhat2
  
  
  J2 <- lm(y ~ x + I(x^2))
  yhat3 <- J2$coef[1] + J2$coef[2] * chemical[i] +J2$coef[3] * chemical[i]^2
  yhat4 <- J2$coef[1] + J2$coef[2] * chemical[j] +J2$coef[3] * chemical[j]^2
  e2[i,j] <- magnetic[i] - yhat3
  e2[j,i] <- magnetic[j] - yhat4

  J3 <- lm(log(y) ~ x)
  logyhat5 <- J3$coef[1] + J3$coef[2] * chemical[i]
  logyhat6 <- J3$coef[1] + J3$coef[2] * chemical[j]
  yhat5 <- exp(logyhat5)
  yhat6 <- exp(logyhat6)
  e3[i,j] <- magnetic[i] - yhat5
  e3[j,i] <- magnetic[j] - yhat6
  
  J4 <- lm(log(y) ~ log(x))
  logyhat7 <- J4$coef[1] + J4$coef[2] * log(chemical[i])
  logyhat8 <- J4$coef[1] + J4$coef[2] * log(chemical[j])
  yhat7 <- exp(logyhat7)
  yhat8 <- exp(logyhat8)
  e4[i,j] <- magnetic[i] - yhat7
  e4[j,i] <- magnetic[j] - yhat8
  }
}
 c(sum(diag(e1%*%t(e1))),sum(diag(e2%*%t(e2))), sum(diag(e3%*%t(e3))),sum(diag(e4%*%t(e4))))/n/(n-1)

```
第二个模型，即二次模型拟合得最好

# homework 7

## Question 1
Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.

## Answer
记$Q$为一转移分布函数,满足对每个$x$,$Q(x,\cdot)$有转移核$q(\cdot|x)$,那么从$Q(x,\cdot)$中抽取一个样本$y$后,计算接受概率$$\alpha(x,y)=min\left\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\right\}.$$产生的马尔科夫链的转移核为$$p(x,y)=q(y|x)\alpha(x,y)=q(y|x)min\left\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\right\}$$所以$$p(x,y)f(x)=f(x)q(y|x)\alpha(x,y)=p(y,x)f(y)$$对细致平衡方程两边积分,则有$$\int{p(x,y)f(x)}\mathrm{d}x=\int{p(y,x)f(y)}\mathrm{d}x\rightarrow f(y)=\int{p(x,y)f(x)}\mathrm{d}x.$$故$f$为平稳分布.

## Question 2
Implement the two-sample Cramer-von Mises test for equal distributions as a
permutation test. Apply the test to the data in Examples 8.1 and 8.2.

## Answer
```{r}
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
R <- 999 
z <- c(x, y) 
K <- 1:26
W <- numeric(R)
cvm<-function(x,y){
  n<-length(x)
  m<-length(y)
  a<-numeric(n)
  b<-numeric(m)
  for(i in 1:n) a[i]<-(sum(x<=x[i])/n-sum(y<=x[i])/m)^2
  for(i in 1:m) b[i]<-(sum(x<=y[i])/n-sum(y<=y[i])/m)^2
  return(n*m*(sum(a)+sum(b))/(m+n)^2)
}
W0 <- cvm(x, y)
for (i in 1:R) {
k <- sample(K, size = 14, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] 
W[i] <- cvm(x1, y1)
}
p <- mean(c(W0, W) >= W0)
p

```

## Question 3
The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

## Answer
```{r}
maxout<- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}
n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 10000 #模拟实验重复次数
R<-999 #重排法的重排次数
K<-1:50
M<-numeric(R)

alphahat <- mean(replicate(m, expr={
  x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) 
y <- y - mean(y)
z<-c(x,y)
M0<-maxout(x,y)
for (i in 1:R) {
k <- sample(K, size = n1, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] 
M[i] <- maxout(x1, y1)
}
p <- mean(c(M0, M) >= M0)
p<=.05
}))
alphahat
```

# homework 8

## Question 1
1. Consider a model $P(Y = 1 | X_1, X_2, X_3) =\frac{exp(a+b_1X_1+b_2X_2+b_3X_3)}{1+exp(a+b_1X_1+b_2X_2+b_3X_3)}$
, where $X_1 ∼ P(1)$, $X_2 ∼ Exp(1)$
and $X_3 ∼ B(1, 0.5)$.  
• Design a function that takes as input values $N$, $b_1$, $b_2$, $b_3$ and $f_0$, and produces the output $a$.  
• Call this function, input values are $N = 106$, $b_1 = 0$, $b_2 = 1$, $b_3 = −1$, $f_0 = 0.1, 0.01, 0.001, 0.0001.$  
• Plot $−log f_0$ vs $a$.

## Answer
```{r }
N <- 1e6; b1 <-0; b2 <- 1; b3<--1;f0 <- c(.1,.01,.001,.0001)
a<-numeric(length(f0))
g <- function(N,b1,b2,b3,f0){
  x1 <- rpois(N,1)
  x2<-rexp(N,1)
  x3 <- rbinom(N,1,0.5)
  f<-function(a){
    tmp <- exp(-a-b1*x1-b2*x2-b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution <- uniroot(f,c(-100,100))
  return(round(unlist(solution)[1],5))
}
for(i in 1:length(f0)){
  a[i]<-g(N,b1,b2,b3,f0[i])
}
a
plot(-log(f0),a)
```

## Question 2  
Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of
each chain.

##Answer
```{r}
 rw.Metropolis <- function( sigma, x0, N) {
       # sigma:  standard variance of proposal distribution N(xt,sigma)
       # x0: initial value
       # N: size of random numbers required.
        x <- numeric(N)
        x[1] <- x0
        u <- runif(N)
        k <- 0
        for (i in 2:N) {
            y <- rnorm(1, x[i-1], sigma)
                if (u[i] <= (exp(-abs(y))) / (exp(-abs(x[i-1])))){
                    x[i] <- y 
                    k <- k + 1
                }
                else 
                    x[i] <- x[i-1]
                  
                
            }
        return(list(x=x, k=k))
        }

    N <- 1e4
    sigma <- c(.05, .5, 2,  16)

    x0 <- 10 
    rw1 <- rw.Metropolis(sigma[1], x0, N)
    rw2 <- rw.Metropolis(sigma[2], x0, N)
    rw3 <- rw.Metropolis(sigma[3], x0, N)
    rw4 <- rw.Metropolis(sigma[4], x0, N)
    print(c(rw1$k, rw2$k, rw3$k, rw4$k)/N) #acceptance rates
    
    par(mfrow=c(2,2))
    rw<-cbind(rw1$x,rw2$x,rw3$x,rw4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",xlab=bquote(sigma == .(round(sigma[j],3))),ylab="X", ylim=range(rw[,j]))
    }
    par(mfrow=c(1,1))

```

## Question 3
Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$
with zero means, unit standard deviations, and correlation 0.9. Plot the
generated sample after discarding a suitable burn-in sample. Fit a simple
linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals
of the model for normality and constant variance.

## Answer
```{r}
  #initialize constants and parameters
    N <- 1e4               #length of chain
    burn <- 2000            #burn-in length
    X <- numeric(N)        #the chain
    Y <- numeric(N)
    rho <-.9             #correlation
    mu1 <- 0
    mu2 <- 0
    sigma1 <- 1
    sigma2 <- 1
    s1 <- sqrt(1-rho^2)*sigma1
    s2 <- sqrt(1-rho^2)*sigma2

    ###### generate the chain #####

    X[1] <- mu1            #initialize
    Y[1] <- mu2
    for (i in 2:N) {
        x2 <- Y[i-1]
        m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
        X[i] <- rnorm(1, m1, s1)
        x1 <- X[i]
        m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
        Y[i] <- rnorm(1, m2, s2)
    }
    b <- burn + 1
    X<- X[b:N]
    Y<- Y[b:N]
    plot(X,Y, main="", cex=.5, xlab=bquote(X),
         ylab=bquote(Y), ylim=range(Y),xlim=range(X))
    
par(mfrow=c(2,2))    
fit<-lm(Y~X,as.data.frame(cbind(Y,X)))
summary(fit)
plot(fit)
```  
  
  由Q-Q图知正态性成立,残差图知方差齐性成立.

## Question 4
Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence
of the chain, and run the chain until the chain has converged approximately to
the target distribution according to $\widehat{R} < 1.2$. (See Exercise 9.9.) Also use the
coda [212] package to check for convergence of the chain by the Gelman-Rubin
method. Hints: See the help topics for the coda functions gelman.diag,
gelman.plot, as.mcmc, and mcmc.list.

## Answer
```{r}
set.seed(12138)
Gelman.Rubin <- function(psi) {
        # psi[i,j] is the statistic psi(X[i,1:j])
        # for chain in i-th row of X
        psi <- as.matrix(psi)
        n <- ncol(psi)
        k <- nrow(psi)

        psi.means <- rowMeans(psi)     #row means
        B <- n * var(psi.means)        #between variance est.
        psi.w <- apply(psi, 1, "var")  #within variances
        W <- mean(psi.w)               #within est.
        v.hat <- W*(n-1)/n + B/n+(B/(n*k))     #upper variance est.
        r.hat <- v.hat / W             #G-R statistic
        return(r.hat)
}

f <- function(x, sigma) {
        if (any(x < 0)) return (0)
        stopifnot(sigma > 0)
        return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

Rayleigh.chain<-function(R,sigma,k){
    x <- matrix(0, nrow=1, ncol=k)
    R.hat<-numeric(1)
    for (i in 1:k) {
      x[1,i] <- rchisq(1, df=1)
    }
    R.hat[1]<-10
    
                                                                                           
      t=2
      while (R.hat[t-1]>=1.2) {
         x<-rbind(x,0)
         R.hat<-rbind(R.hat,0)
        for(i in 1:k){
          xt <- x[t-1,i]
          y <- rchisq(1, df = xt)
          num <- f(y, sigma) * dchisq(xt, df = y)
          den <- f(xt, sigma) * dchisq(y, df = xt)
          u <- runif(k)
          if (u[i] <= num/den) x[t,i] <- y 
          else  x[t,i] <- xt
        }
        psi <- t(apply(x, 2, cumsum))
        for (i in 1:nrow(psi))
        psi[i,] <- psi[i,] / (1:ncol(psi))
        R.hat[t]<-Gelman.Rubin(psi)
        t<-t+1
      }

    return(list(GR.statistic=R.hat[t-1],chain=x,length=t-1) )
 
  
}
    sigma <- 4     #parameter of proposal distribution
    k <- 4          #number of chains to generate
    R <- 1.2     #target value of Gelman-Rubin statistic 
    

a<-Rayleigh.chain(R,sigma,k)
a$GR.statistic


###### coda  package #####
library(coda)
b<-mcmc.list(chain1=mcmc(a$chain[,1]),chain2=mcmc(a$chain[,2]),chain3=mcmc(a$chain[,3]),chain4=mcmc(a$chain[,4]))
gelman.diag(b)
gelman.plot(b)
```

# homework 9

## Question 1

设$X_1,...,X_n\sim Exp(\lambda),i.i.d$因为某种原因,只知道$X_i$落在某个区间$(U_i,V_i)$,其中$U_i<V_i$是两个非随机的已知常数.这种数据称为区间删失数据.  
(1)试分别直接极大化观测数据的似然函数与采用EM算法求解$\lambda$的MLE,证明EM算法收敛于观察数据的MLE,且收敛有线性速度.  
(2)设$(U_i,V_i),i=1,...,n(=10)$的观测值为(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3),试分别编程实现上述两种算法以得到$\lambda$的MLE的数值解.

## Answer

MLE:似然函数$L(\lambda)=\prod_{i=1}^{n}P(u_i \leqslant X_i \leqslant v_i)=\prod_{i=1}^{n}\mathrm e^{-\lambda u_i}-\mathrm e^{-\lambda v_i}$  
要极大化$L(\lambda)$,即求解$$\sum_{i=1}^{n} \frac{v_i\mathrm e^{-\lambda v_i}-u_i\mathrm e^{-\lambda u_i}}{\mathrm e^{-\lambda u_i}-\mathrm e^{-\lambda v_i}}=0$$ 
EM:设完整数据为$x_i,i=1,...,n$  
$l(\lambda|x_i)=n\log\lambda-\lambda*\sum_{i=1}^{n}x_i$  
$E_{\lambda_0}[l(\lambda|x_i)|u_i,v_i]=n\log\lambda-\lambda(\sum_{i=1}^{n} \frac{u_i\mathrm e^{-\lambda_0 u_i}-v_i\mathrm e^{-\lambda_0 v_i}}{\mathrm e^{-\lambda_0 u_i}-\mathrm e^{-\lambda_0 v_i}}+\frac{1}{\lambda_0})$  
极大化上式得:$$\lambda_1=\frac{n}{\sum_{i=1}^{n} (\frac{u_i\mathrm e^{-\lambda_0 u_i}-v_i\mathrm e^{-\lambda_0 v_i}}{\mathrm e^{-\lambda_0 u_i}-\mathrm e^{-\lambda_0 v_i}}+\frac{1}{\lambda_0})}$$  
收敛速度: 令$f(\lambda)=\sum_{i=1}^{n} \frac{u_i\mathrm e^{-\lambda u_i}-v_i\mathrm e^{-\lambda v_i}}{\mathrm e^{-\lambda u_i}-\mathrm e^{-\lambda v_i}}$  
$$\begin{align}
\lim_{k \to \infty}|\frac{\lambda_{k+1}-\lambda_{MLE}}{\lambda_{k}-\lambda_{MLE}}|&=\lim_{k \to \infty}|\frac{\frac{n}{f(\lambda_k)+n/\lambda_k}-\lambda_{MLE}}{\lambda_{k}-\lambda_{MLE}}|\\&=\lim_{k \to \infty}1-\frac{\lambda_k\lambda_{MLE}}{\lambda_kf(\lambda_k)+1}f'(\lambda_k)\\&=1
\end{align}$$




```{r}
u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)
n<-10
f<-function(lambda){
  a<--exp(-lambda*u)/(exp(-lambda*u)-exp(-lambda*v))
  b<-exp(-lambda*v)/(exp(-lambda*u)-exp(-lambda*v))
  t(a)%*%u+t(b)%*%v
}
solution <- uniroot(f,c(.01,10))
MLE <- solution$root

em<-function(lambda,m){
  a<-numeric(m)
  a[1]<-lambda
  for(i in 2:m){
    b<-sum((u*exp(-a[i-1]*u)-v*exp(-a[i-1]*v))/(exp(-a[i-1]*u)-exp(-a[i-1]*v))+1/a[i-1])
    a[i]<-n/b
  }
  return(a[m])
}
EM<-em(MLE,1e5)

round(c(MLE,EM),5)
```

## Question 2
In the Morra game, the set of optimal strategies are not changed if a constant
is subtracted from every entry of the payoff matrix, or a positive constant
is multiplied times every entry of the payoff matrix. However, the simplex
algorithm may terminate at a different basic feasible point (also optimal).
Compute B <- A + 2, find the solution of game B, and verify that it is one
of the extreme points (11.12)–(11.15) of the original game A. Also find the
value of game A and game B.

## Answer

```{r}
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
library(boot) 
B<-A+2

m<-nrow(B)
n<-ncol(B)
a <- c(rep(0, m), 1) 
A1 <- -cbind(t(B), rep(-1, n)) 
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) 
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE)
sx$soln
```
the solution of game B is (11.14),the value of B is 2,the value of A is 0

# homework 10

## Question 1
Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?

## Answer
list也是一种vector,故只能用unlist转化为atomic vector.

## Question 2
What does dim() return when applied to a vector?

## Answer
NULL

## Question 3
If is.matrix(x) is TRUE, what will is.array(x) return?

## Answer
TRUE

## Question 4
What does as.matrix() do when applied to a data frame with
columns of different types?

## Answer
所有元素变为最高阶的类型.
logical < integer < double < character < list

## Question 5
Can you have a data frame with 0 rows? What about 0
columns

## Answer

```{r}
a<-data.frame("x"=NULL)
a
```
## Question 6
The function below scales a vector so it falls in the range [0,
1]. How would you apply it to every column of a data frame?
How would you apply it to every numeric column in a data
frame?
```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```

## Answer

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
a<-data.frame(x=c(1,2,3),y=c("a","b","c"))
b<-lapply(a,is.numeric)
scale01(a[which(unlist(b))])
```

## Question 7
Use vapply() to:
a) Compute the standard deviation of every column in a numeric data frame.
b) Compute the standard deviation of every numeric column
in a mixed data frame. (Hint: you’ll need to use vapply()
twice.)

## Answer

```{r}
a<-data.frame(x=c(1,2,3),y=c(4,5,6))
vapply(a,sd,numeric(1))

b<-data.frame(x=c(1,2,3),y=c("a","b","c"))
c<-vapply(b,is.numeric,logical(1))
vapply(b[which(c)],sd,numeric(1))
```

## Question 8
Consider Exercise 9.8 (pages 278, Statistical Computing with R). (Hint: Refer to the first example of Case studies section)
Consider the bivariate density
$$f(x, y)\varpropto \binom{n}{x} y^{x+a−1}(1 − y)^{n-x+b-1},x=0,1,...,n,0\leqslant y \leqslant 1$$
It can be shown (see e.g. [23]) that for fixed $a, b, n$, the conditional distributions are $ Binomial(n, y)$ and $Beta(x + a, n − x + b)$. Use the Gibbs sampler to
generate a chain with target joint density $f(x, y)$.  
• Write an R function.  
• Write an Rcpp function.  
• Compare the computation time of the two functions with the function “microbenchmark”.

## Answer

```{r}
N <- 5000 
a<-1
b<-1
n<-5  
Gibbs<-function(N,a,b,n){
  X <- matrix(0,N,2)
  X[1, ] <- c(0, 0) 
  for (i in 2:N) {
    y <- X[i-1, 2]
    X[i, 1] <- rbinom(1, n, y)
    x <- X[i, 1]
    X[i, 2] <- rbeta(1, x+a, n-x+b)
  }
return(X)
}

```
```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;

//[[Rcpp::export]]
NumericVector GibbsCpp(int N, double a, double b, double n){
  NumericMatrix X(N,2);
  X(0,0) = 0 ;
  X(0,1) = 0 ;
  for(int i=1; i <= N; i++){
    double y = X(i-1, 1) ;
    X(i, 0) = ::Rf_rbinom( n, y) ;
    double x = X(i, 0) ;
    X(i, 1) = ::Rf_rbeta( x+a, n-x+b) ;
}

  return X;
}

```
```{r}
library(microbenchmark)
ts <- microbenchmark(gibbR=Gibbs(N,a,b,n),gibbC=GibbsCpp(N,a,b,n))
summary(ts)[,c(1,3,5,6)]
```